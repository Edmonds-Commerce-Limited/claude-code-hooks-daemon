# Plan 002: Fix Silent Handler Failures

**Status**: ğŸŸ¡ In Progress
**Created**: 2026-01-27
**Owner**: TBD
**Priority**: Critical
**Estimated Effort**: 8-12 hours

## Overview

Fix critical silent failures where handlers match events but fail to process them due to wrong field names, then add robust input validation at the front controller layer to prevent this class of bug from recurring. This addresses the findings from Plan 001 (Test Fixture Validation).

**Problem**: Multiple handlers are silently broken in production:
- BashErrorDetectorHandler uses `tool_output` (real events have `tool_response`)
- AutoApproveReadsHandler uses `permission_type` (real events have `permission_suggestions`)
- NotificationLoggerHandler tests use `severity` (real events have `notification_type`)

**Impact**: Handlers match events, return success, but never actually process data. No errors logged. Claude Code receives success responses. Users have no indication anything is wrong.

**Architecture**: Input validation will be added at the **outermost front controller layer** (server.py `_handle_client()`) to validate hook_input ONCE per event BEFORE dispatching to handlers. This ensures all handlers receive validated data.

## Goals

- âœ… Fix all broken handlers to use correct field names
- âœ… Update all test fixtures to match real Claude Code event structures
- âœ… Add input schema validation (toggleable via config/env var)
- âœ… Add sanity checks for required fields
- âœ… Ensure handlers fail loudly when data is missing
- âœ… Maintain 95%+ test coverage
- âœ… Document input validation patterns for future handlers

## Non-Goals

- âŒ Change fail-open architecture (that's by design for hook resilience)
- âŒ Add validation that impacts performance significantly (< 5ms overhead target)
- âŒ Rewrite all handlers (only fix broken ones)
- âŒ Validate response schemas (already exists)

## Context & Background

From Plan 001, we discovered:

1. **PostToolUse handlers**: Use `tool_output` (wrong), should use `tool_response` (correct)
2. **PermissionRequest handlers**: Use `permission_type`/`resource` (wrong), should use `permission_suggestions` (correct)
3. **Notification tests**: Use `severity` (wrong), should use `notification_type` (correct)

**Root Causes**:
- Handlers written based on assumptions, not real event captures
- No input schema validation (only response schemas exist)
- `.get()` with defaults masks missing required fields
- Fail-open architecture allows silent failures

**Why No Errors**: Handlers use `.get("wrong_field", {})` which returns empty dict, then immediately return ALLOW with no error logging.

See `/workspace/CLAUDE/Plan/002-fix-silent-handler-failures/CRITICAL_ANALYSIS_SILENT_FAILURES.md` for complete analysis.

## Tasks

### Phase 1: Design Input Validation System
- [ ] â¬œ **Task 1.1**: Research validation approaches
  - [ ] â¬œ Evaluate jsonschema performance (benchmark with 10k events)
  - [ ] â¬œ Compare: full schemas vs sanity checks vs hybrid
  - [ ] â¬œ **ARCHITECTURE**: Validate at outermost layer (server.py) ONCE per event
  - [ ] â¬œ Design error handling strategy
- [ ] â¬œ **Task 1.2**: Design input schemas structure
  - [ ] â¬œ Create `input_schemas.py` alongside `response_schemas.py`
  - [ ] â¬œ Define schemas for ALL event types (PreToolUse, PostToolUse, PermissionRequest, etc.)
  - [ ] â¬œ Document required vs optional fields per event type
  - [ ] â¬œ Include tool-specific structures (Bash, Read, Write, etc.)
  - [ ] â¬œ Define validation error format returned to Claude Code
- [ ] â¬œ **Task 1.3**: Design configuration approach
  - [ ] â¬œ Add `validate_input` boolean to DaemonConfig (default: False initially)
  - [ ] â¬œ Add `HOOKS_DAEMON_VALIDATE_INPUT` env var override
  - [ ] â¬œ Document in config schema and init_config.py
  - [ ] â¬œ Add validation toggle to CLI commands

### Phase 2: Implement Input Validation (TDD)
- [ ] â¬œ **Task 2.1**: Create input schemas
  - [ ] â¬œ Write tests for schema structure
  - [ ] â¬œ Implement PreToolUse input schema
  - [ ] â¬œ Implement PostToolUse input schema (tool_response structure)
  - [ ] â¬œ Implement PermissionRequest input schema (permission_suggestions)
  - [ ] â¬œ Implement Notification input schema (notification_type)
  - [ ] â¬œ Implement remaining event schemas
- [ ] â¬œ **Task 2.2**: Implement validation function
  - [ ] â¬œ Write tests for validate_input()
  - [ ] â¬œ Implement validate_input(event_type, hook_input)
  - [ ] â¬œ Return validation errors list
  - [ ] â¬œ Add performance logging
- [ ] â¬œ **Task 2.3**: Integrate into server.py front controller layer
  - [ ] â¬œ Write tests for server validation integration
  - [ ] â¬œ Add validation call in _handle_client() after parsing JSON, BEFORE dispatch
  - [ ] â¬œ Validate ONCE per event at outermost layer (not in handlers/chain)
  - [ ] â¬œ Return error response to Claude Code if validation fails
  - [ ] â¬œ Log validation failures at WARNING level with full details
  - [ ] â¬œ Add metrics tracking (validation_failures counter)
  - [ ] â¬œ Short-circuit: Don't dispatch to handlers if validation fails

### Phase 3: Fix BashErrorDetectorHandler (PostToolUse)
- [ ] â¬œ **Task 3.1**: Update handler implementation
  - [ ] â¬œ Write failing tests using tool_response field
  - [ ] â¬œ Change tool_output â†’ tool_response in handler
  - [ ] â¬œ Remove exit_code dependency (doesn't exist in real events)
  - [ ] â¬œ Use stderr/stdout content for error detection
  - [ ] â¬œ Implement tests pass
- [ ] â¬œ **Task 3.2**: Update all PostToolUse test fixtures
  - [ ] â¬œ Update test_bash_error_detector.py fixtures (tool_output â†’ tool_response)
  - [ ] â¬œ Remove exit_code from fixtures
  - [ ] â¬œ Add stdout, stderr, interrupted, isImage fields
  - [ ] â¬œ Update test_validate_eslint_on_write.py fixtures
  - [ ] â¬œ Update test_validate_sitemap.py fixtures
  - [ ] â¬œ Update integration test fixtures
- [ ] â¬œ **Task 3.3**: Verify handler works in production
  - [ ] â¬œ Test with debug_hooks.sh capturing real events
  - [ ] â¬œ Verify handler matches and processes data
  - [ ] â¬œ Verify error detection actually works

### Phase 4: Fix AutoApproveReadsHandler (PermissionRequest)
- [ ] â¬œ **Task 4.1**: Redesign handler for correct structure
  - [ ] â¬œ Analyze real permission_suggestions structure from logs
  - [ ] â¬œ Decide: Keep auto-approve concept or repurpose handler?
  - [ ] â¬œ Write tests for new matching logic (permission_suggestions based)
  - [ ] â¬œ Reimplement matches() to check permission_suggestions
  - [ ] â¬œ Reimplement handle() to return appropriate result
- [ ] â¬œ **Task 4.2**: Update all PermissionRequest test fixtures
  - [ ] â¬œ Update test_auto_approve_reads.py (147 tests)
  - [ ] â¬œ Change permission_type/resource â†’ permission_suggestions structure
  - [ ] â¬œ Add tool_name, tool_input fields
  - [ ] â¬œ Update integration test fixtures
  - [ ] â¬œ Verify all 147 tests still pass
- [ ] â¬œ **Task 4.3**: Verify handler works in production
  - [ ] â¬œ Trigger permission request in Claude Code
  - [ ] â¬œ Verify handler matches in logs
  - [ ] â¬œ Verify handler processes permission_suggestions

### Phase 5: Fix NotificationLoggerHandler Tests
- [ ] â¬œ **Task 5.1**: Update test fixtures
  - [ ] â¬œ Update test_notification_logger.py (severity â†’ notification_type)
  - [ ] â¬œ Use documented types: permission_prompt, idle_prompt, auth_success
  - [ ] â¬œ Remove invalid fields: code, details
  - [ ] â¬œ Add standard fields: session_id, transcript_path, cwd
  - [ ] â¬œ Update integration test fixtures
- [ ] â¬œ **Task 5.2**: Verify handler still works
  - [ ] â¬œ Handler is generic (passes through all fields)
  - [ ] â¬œ Verify logs contain correct field names
  - [ ] â¬œ All tests passing

### Phase 6: Add Defensive Checks to Handlers (Optional)
- [ ] â¬œ **Task 6.1**: Define defensive coding pattern for handlers
  - [ ] â¬œ Document: Handlers can trust validated input from front controller
  - [ ] â¬œ Document: When to add defensive checks vs trusting validation
  - [ ] â¬œ Create handler_utils.py with helper functions if needed
  - [ ] â¬œ Update HANDLER_DEVELOPMENT.md with best practices
- [ ] â¬œ **Task 6.2**: Review critical handlers for defensive improvements
  - [ ] â¬œ BashErrorDetector: Verify assumes tool_response exists
  - [ ] â¬œ DestructiveGit: Verify assumes tool_input.command exists
  - [ ] â¬œ Document which fields are guaranteed by validation vs optional

**Note**: With front controller validation, handlers can mostly trust input structure. Defensive checks are for edge cases not caught by schema (e.g., empty strings where non-empty expected).

### Phase 7: Performance & Integration Testing
- [ ] â¬œ **Task 7.1**: Benchmark validation performance
  - [ ] â¬œ Create benchmark script with 10k events
  - [ ] â¬œ Measure baseline (no validation)
  - [ ] â¬œ Measure with validation enabled
  - [ ] â¬œ Target: < 5ms overhead per event
  - [ ] â¬œ Document results
- [ ] â¬œ **Task 7.2**: Integration testing with live Claude Code
  - [ ] â¬œ Enable validation: HOOKS_DAEMON_VALIDATE_INPUT=true
  - [ ] â¬œ Run through common workflows (git, file ops, agent tasks)
  - [ ] â¬œ Verify no validation failures for valid events
  - [ ] â¬œ Verify validation catches malformed events
  - [ ] â¬œ Check daemon logs for any issues
- [ ] â¬œ **Task 7.3**: Full QA suite
  - [ ] â¬œ Run ./scripts/qa/run_all.sh
  - [ ] â¬œ Verify 95%+ coverage maintained
  - [ ] â¬œ All 2484+ tests passing
  - [ ] â¬œ No type errors
  - [ ] â¬œ No security issues

### Phase 8: Documentation & Completion
- [ ] â¬œ **Task 8.1**: Update documentation
  - [ ] â¬œ Update CLAUDE/HANDLER_DEVELOPMENT.md with sanity check patterns
  - [ ] â¬œ Document input validation in DAEMON.md
  - [ ] â¬œ Add input_schemas.py documentation
  - [ ] â¬œ Update README.md configuration section
- [ ] â¬œ **Task 8.2**: Update default configuration
  - [ ] â¬œ Decide: Enable validation by default or opt-in?
  - [ ] â¬œ Update .claude/hooks-daemon.yaml template
  - [ ] â¬œ Update init_config.py default value
  - [ ] â¬œ Document in UPGRADES/ if breaking change
- [ ] â¬œ **Task 8.3**: Commit and push
  - [ ] â¬œ Review all changes
  - [ ] â¬œ Create comprehensive commit message
  - [ ] â¬œ Push to origin/main
  - [ ] â¬œ Mark plan as complete

## Dependencies

- **Depends on**: Plan 001 (Complete) - Provides analysis and verification reports
- **Blocks**: None yet
- **Related**: None yet

## Technical Decisions

### Decision 1: Validation Approach
**Context**: Need to prevent silent failures without impacting performance
**Options Considered**:
1. Full jsonschema validation (comprehensive but potentially slow)
2. Sanity checks only (fast but less comprehensive)
3. Hybrid: Sanity checks always + optional full validation

**Decision**: TBD - Need performance benchmarks
**Factors**:
- jsonschema validation cost vs benefit
- Whether validation should be opt-in or opt-out
- Impact on daemon latency

### Decision 2: Configuration Default
**Context**: Should validation be enabled by default?
**Options Considered**:
1. Opt-in (validate_input: false by default) - safer rollout
2. Opt-out (validate_input: true by default) - better protection

**Decision**: TBD - Depends on performance results
**Factors**:
- Performance impact
- Backward compatibility
- User experience

### Decision 3: Validation Layer Location
**Context**: Where should input validation happen?
**Options Considered**:
1. In individual handlers (validate per-handler) - Flexible but inefficient, validated N times
2. In handler chain (validate per-chain) - Better but still after routing
3. In front controller server.py (validate once per event) - Outermost layer, most efficient

**Decision**: Front controller layer (server.py `_handle_client()`) - ARCHITECTURAL DECISION
**Rationale**:
- Validate ONCE per event, not N times per handler
- Catch invalid data before any handler processing
- Single source of truth for valid event structure
- Fail fast at system boundary
- All handlers can trust input is valid
**Date**: 2026-01-27

### Decision 4: Error Handling
**Context**: What happens when validation fails?
**Options Considered**:
1. Return error to Claude Code (fail-closed for validation)
2. Log error and allow (fail-open for validation)
3. Configurable behavior

**Decision**: TBD - Need to understand Claude Code's error handling
**Factors**:
- Impact on user experience
- Claude Code's retry behavior
- Debugging experience
**Note**: Validation failures are different from handler failures - validation means malformed input from Claude Code itself

## Success Criteria

- [ ] All broken handlers fixed (BashErrorDetector, AutoApproveReads, Notification tests)
- [ ] All test fixtures match real Claude Code event structures
- [ ] Input validation system implemented and configurable
- [ ] Sanity check pattern documented and implemented
- [ ] Performance overhead < 5ms per event (with validation enabled)
- [ ] All tests passing (2484+ tests)
- [ ] Coverage maintained at 95%+
- [ ] No type errors (MyPy strict mode)
- [ ] Handlers verified working in live Claude Code sessions
- [ ] Documentation updated

## Risks & Mitigations

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Validation too slow | High | Medium | Benchmark early, make validation optional, optimize schemas |
| Breaking existing handlers | High | Low | Comprehensive testing, gradual rollout, opt-in initially |
| Validation too strict | Medium | Medium | Test with real events, allow optional fields, document schema |
| Complex handler redesign | Medium | High | Focus on fixing field names first, redesign only if needed |
| Coverage drops below 95% | Medium | Low | Write tests first (TDD), monitor coverage continuously |

## Timeline

- **Phase 1 (Design)**: 2 hours
- **Phase 2 (Validation System)**: 4 hours
- **Phase 3 (BashErrorDetector)**: 2 hours
- **Phase 4 (AutoApproveReads)**: 2 hours
- **Phase 5 (Notification)**: 1 hour
- **Phase 6 (Sanity Checks)**: 2 hours
- **Phase 7 (Testing)**: 2 hours
- **Phase 8 (Documentation)**: 1 hour
- **Target Completion**: 2026-01-28 (allowing buffer for issues)

## Notes & Updates

### 2026-01-27
- Plan created based on Plan 001 findings
- Moved CRITICAL_ANALYSIS_SILENT_FAILURES.md into plan folder
- Awaiting Opus agent research on validation approach
- Need to decide on validation strategy before implementation begins

## Artifacts

Located in `/workspace/CLAUDE/Plan/002-fix-silent-handler-failures/`:

1. **CRITICAL_ANALYSIS_SILENT_FAILURES.md** - Comprehensive analysis of the problem
2. **PLAN.md** (this file) - Implementation plan

## Reference Documentation

- Plan 001: Test Fixture Validation Against Real Claude Code Events
- CLAUDE/HANDLER_DEVELOPMENT.md - Handler patterns
- CLAUDE/DEBUGGING_HOOKS.md - How to capture real events
- src/claude_code_hooks_daemon/core/response_schemas.py - Example validation approach
